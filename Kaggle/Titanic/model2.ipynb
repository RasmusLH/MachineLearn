{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/titanic/train.csv')\n",
    "test = pd.read_csv('Data/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(name):\n",
    "    if pd.isnull(name):\n",
    "        return 'Unknown'\n",
    "    return name.split(',')[1].split('.')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Title'] = train['Name'].apply(extract_title)\n",
    "test['Title'] = test['Name'].apply(extract_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate rare titles\n",
    "rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \n",
    "               'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']\n",
    "train['Title'] = train['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "train['Title'] = train['Title'].replace('Mme', 'Mrs')\n",
    "train['Title'] = train['Title'].apply(lambda x: 'Rare' if x in rare_titles else x)\n",
    "test['Title'] = test['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
    "test['Title'] = test['Title'].replace('Mme', 'Mrs')\n",
    "test['Title'] = test['Title'].apply(lambda x: 'Rare' if x in rare_titles else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FamilySize feature\n",
    "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
    "\n",
    "# Create a feature to indicate if passenger had family onboard\n",
    "train['IsAlone'] = 1*(train['FamilySize'] == 1)\n",
    "test['IsAlone'] = 1*(test['FamilySize'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that likely add little value to our prediction\n",
    "drop_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
    "train_features = train.drop(columns=drop_columns + ['Survived'])\n",
    "test_features = test.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical features, we'll perform label encoding on:\n",
    "# 'Sex', 'Embarked', and our engineered 'Title'\n",
    "categorical_features = ['Sex', 'Embarked', 'Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a simple imputer for numeric features (Age, Fare) as well\n",
    "numeric_features = ['Age', 'Fare', 'FamilySize', 'SibSp', 'Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing numerical values with median (imputation)\n",
    "for col in numeric_features:\n",
    "    median_val = train_features[col].median()\n",
    "    train_features[col] = train_features[col].fillna(median_val)\n",
    "    if col in test_features.columns:\n",
    "        test_features[col] = test_features[col].fillna(train_features[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Embarked, fill missing values with mode from train set\n",
    "for col in ['Embarked']:\n",
    "    mode_val = train_features[col].mode()[0]\n",
    "    train_features[col] = train_features[col].fillna(mode_val)\n",
    "    if col in test_features.columns:\n",
    "        test_features[col] = test_features[col].fillna(mode_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding for categorical variables\n",
    "encoder = LabelEncoder()\n",
    "for col in categorical_features:\n",
    "    # Fit on train and then transform both datasets to ensure consistency\n",
    "    train_features[col] = encoder.fit_transform(train_features[col])\n",
    "    test_features[col] = encoder.transform(test_features[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features\n",
    "y_train = train['Survived']\n",
    "X_test = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use RandomForestClassifier with a basic grid search for hyperparameters.\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 400],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 400}\n",
      "CV accuracy scores: [0.84357542 0.80898876 0.87078652 0.80898876 0.84269663]\n",
      "Mean CV accuracy: 0.8350072186303434\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# Cross-validation score (for our own reference)\n",
    "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5)\n",
    "print(\"CV accuracy scores:\", cv_scores)\n",
    "print(\"Mean CV accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure PassengerId order from test data is maintained\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test[\"PassengerId\"],\n",
    "    \"Survived\": predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission4.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv(\"submission4.csv\", index=False)\n",
    "print(\"Submission file 'submission4.csv' created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
